{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mPackage MNIST is already installed\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mMETADATA is out-of-date — you may not have the latest version of MNIST\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mUse `Pkg.update()` to get the latest versions of your packages\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mArray{T}(::Type{T}, m::Int, n::Int) is deprecated, use Array{T}(m, n) instead.\u001b[39m\n",
      "Stacktrace:\n",
      " [1] \u001b[1mdepwarn\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::Symbol\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\deprecated.jl:70\u001b[22m\u001b[22m\n",
      " [2] \u001b[1mArray\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Type{Float64}, ::Int64, ::Int64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\deprecated.jl:57\u001b[22m\u001b[22m\n",
      " [3] \u001b[1mtraindata\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1mC:\\Users\\User\\AppData\\Local\\JuliaPro-0.6.0.1\\pkgs-0.6.0.1\\v0.6\\MNIST\\src\\MNIST.jl:88\u001b[22m\u001b[22m\n",
      " [4] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\loading.jl:515\u001b[22m\u001b[22m\n",
      " [5] \u001b[1mexecute_request\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m\u001b[22m at \u001b[1mC:\\Users\\User\\AppData\\Local\\JuliaPro-0.6.0.1\\pkgs-0.6.0.1\\v0.6\\IJulia\\src\\execute_request.jl:160\u001b[22m\u001b[22m\n",
      " [6] \u001b[1meventloop\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m\u001b[22m at \u001b[1mC:\\Users\\User\\AppData\\Local\\JuliaPro-0.6.0.1\\pkgs-0.6.0.1\\v0.6\\IJulia\\src\\eventloop.jl:8\u001b[22m\u001b[22m\n",
      " [7] \u001b[1m(::IJulia.##11#14)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\task.jl:335\u001b[22m\u001b[22m\n",
      "while loading In[2], in expression starting on line 2\n",
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mArray{T}(::Type{T}, m::Int) is deprecated, use Array{T}(m) instead.\u001b[39m\n",
      "Stacktrace:\n",
      " [1] \u001b[1mdepwarn\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::Symbol\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\deprecated.jl:70\u001b[22m\u001b[22m\n",
      " [2] \u001b[1mArray\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Type{Float64}, ::Int64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\deprecated.jl:57\u001b[22m\u001b[22m\n",
      " [3] \u001b[1mtraindata\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1mC:\\Users\\User\\AppData\\Local\\JuliaPro-0.6.0.1\\pkgs-0.6.0.1\\v0.6\\MNIST\\src\\MNIST.jl:89\u001b[22m\u001b[22m\n",
      " [4] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\loading.jl:515\u001b[22m\u001b[22m\n",
      " [5] \u001b[1mexecute_request\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m\u001b[22m at \u001b[1mC:\\Users\\User\\AppData\\Local\\JuliaPro-0.6.0.1\\pkgs-0.6.0.1\\v0.6\\IJulia\\src\\execute_request.jl:160\u001b[22m\u001b[22m\n",
      " [6] \u001b[1meventloop\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m\u001b[22m at \u001b[1mC:\\Users\\User\\AppData\\Local\\JuliaPro-0.6.0.1\\pkgs-0.6.0.1\\v0.6\\IJulia\\src\\eventloop.jl:8\u001b[22m\u001b[22m\n",
      " [7] \u001b[1m(::IJulia.##11#14)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\task.jl:335\u001b[22m\u001b[22m\n",
      "while loading In[2], in expression starting on line 2\n",
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mArray{T}(::Type{T}, m::Int, n::Int) is deprecated, use Array{T}(m, n) instead.\u001b[39m\n",
      "Stacktrace:\n",
      " [1] \u001b[1mdepwarn\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::Symbol\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\deprecated.jl:70\u001b[22m\u001b[22m\n",
      " [2] \u001b[1mArray\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Type{Float64}, ::Int64, ::Int64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\deprecated.jl:57\u001b[22m\u001b[22m\n",
      " [3] \u001b[1mtestdata\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1mC:\\Users\\User\\AppData\\Local\\JuliaPro-0.6.0.1\\pkgs-0.6.0.1\\v0.6\\MNIST\\src\\MNIST.jl:99\u001b[22m\u001b[22m\n",
      " [4] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\loading.jl:515\u001b[22m\u001b[22m\n",
      " [5] \u001b[1mexecute_request\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m\u001b[22m at \u001b[1mC:\\Users\\User\\AppData\\Local\\JuliaPro-0.6.0.1\\pkgs-0.6.0.1\\v0.6\\IJulia\\src\\execute_request.jl:160\u001b[22m\u001b[22m\n",
      " [6] \u001b[1meventloop\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m\u001b[22m at \u001b[1mC:\\Users\\User\\AppData\\Local\\JuliaPro-0.6.0.1\\pkgs-0.6.0.1\\v0.6\\IJulia\\src\\eventloop.jl:8\u001b[22m\u001b[22m\n",
      " [7] \u001b[1m(::IJulia.##11#14)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\task.jl:335\u001b[22m\u001b[22m\n",
      "while loading In[2], in expression starting on line 3\n",
      "\u001b[1m\u001b[33mWARNING: \u001b[39m\u001b[22m\u001b[33mArray{T}(::Type{T}, m::Int) is deprecated, use Array{T}(m) instead.\u001b[39m\n",
      "Stacktrace:\n",
      " [1] \u001b[1mdepwarn\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::Symbol\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\deprecated.jl:70\u001b[22m\u001b[22m\n",
      " [2] \u001b[1mArray\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Type{Float64}, ::Int64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\deprecated.jl:57\u001b[22m\u001b[22m\n",
      " [3] \u001b[1mtestdata\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1mC:\\Users\\User\\AppData\\Local\\JuliaPro-0.6.0.1\\pkgs-0.6.0.1\\v0.6\\MNIST\\src\\MNIST.jl:100\u001b[22m\u001b[22m\n",
      " [4] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\loading.jl:515\u001b[22m\u001b[22m\n",
      " [5] \u001b[1mexecute_request\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m\u001b[22m at \u001b[1mC:\\Users\\User\\AppData\\Local\\JuliaPro-0.6.0.1\\pkgs-0.6.0.1\\v0.6\\IJulia\\src\\execute_request.jl:160\u001b[22m\u001b[22m\n",
      " [6] \u001b[1meventloop\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m\u001b[22m at \u001b[1mC:\\Users\\User\\AppData\\Local\\JuliaPro-0.6.0.1\\pkgs-0.6.0.1\\v0.6\\IJulia\\src\\eventloop.jl:8\u001b[22m\u001b[22m\n",
      " [7] \u001b[1m(::IJulia.##11#14)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\task.jl:335\u001b[22m\u001b[22m\n",
      "while loading In[2], in expression starting on line 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000-element Array{Int64,1}:\n",
       " 7\n",
       " 2\n",
       " 1\n",
       " 0\n",
       " 4\n",
       " 1\n",
       " 4\n",
       " 9\n",
       " 5\n",
       " 9\n",
       " 0\n",
       " 6\n",
       " 9\n",
       " ⋮\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MNIST\n",
    "trainX, trainY = traindata()\n",
    "testX, testY = testdata()\n",
    "trainY=Int.(trainY)\n",
    "testY=Int.(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type NeuralNetwork\n",
    "    numOfLayers::Int\n",
    "    sizes::Array{Int,1}\n",
    "    biases::Array{Array{Float64,1},1}\n",
    "    weights::Array{Array{Float64,2},1}\n",
    "    function NeuralNetwork(s)\n",
    "        numOfLayers=length(s)\n",
    "        sizes=s\n",
    "        biases=[rand(y) for y in s[2:end]]\n",
    "        weights=[rand(y,x) for (x,y) in zip(s[1:end-1],s[2:end])]\n",
    "        new(numOfLayers,sizes,biases,weights)\n",
    "    #return biases, weights\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(3, [784, 15, 10], Array{Float64,1}[[0.513886, 0.000423805, 0.962601, 0.169844, 0.389471, 0.676696, 0.677257, 0.149039, 0.126735, 0.242021, 0.501019, 0.20059, 0.285737, 0.695962, 0.244196], [0.454473, 0.484908, 0.90145, 0.120237, 0.131669, 0.592903, 0.209748, 0.593931, 0.482784, 0.909551]], Array{Float64,2}[[0.138632 0.503996 … 0.00588135 0.248403; 0.716476 0.113758 … 0.354183 0.370511; … ; 0.401057 0.688614 … 0.42302 0.609837; 0.304336 0.124245 … 0.916517 0.94341], [0.234465 0.923546 … 0.203764 0.878007; 0.304044 0.821021 … 0.557493 0.217041; … ; 0.786513 0.56555 … 0.134947 0.172172; 0.0559474 0.833103 … 0.286756 0.301428]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NeuralNetwork([784,15,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoid (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sigmoid(z)\n",
    "  return 1.0 / (1.0 + exp(-z))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feedforward (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function feedforward(net,a)\n",
    "    for (b,w) in zip(net.biases[1:end-1], net.weights[1:end-1])\n",
    "        a = sigmoid.((w*a)+b)\n",
    "    end\n",
    "   #d is the sum over all output neurons\n",
    "    d=0\n",
    "    for k in a\n",
    "        d+=exp(k)\n",
    "    end\n",
    "    for (b,w) in zip(net.biases[end], net.weights[end])\n",
    "        a = softmax.((w*a)+b,d)\n",
    "    end\n",
    "    #println(sum(a))\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(z,d) \n",
    "    return exp(z)/d\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function SGD(net, training_data, epochs, mini_batch_size, eta,test_data)\n",
    "    n_test = length(test_data)\n",
    "    n = length(training_data)\n",
    "    training_data2=collect(training_data)\n",
    "    mini_batches = [training_data2[k:k+(mini_batch_size-1)] for k=1:mini_batch_size:n-1]\n",
    "    for j in 1:epochs\n",
    "        for mini_batch in mini_batches\n",
    "            update_mini_batch(net, mini_batch, eta)\n",
    "        end\n",
    "       \n",
    "        print(\"epoch \",j, \": \",evaluate(net,test_data),\"  \")\n",
    "        \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_mini_batch (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_mini_batch(net, mini_batch, eta)\n",
    "    #def fn(a, y):\n",
    "     #   return np.sum(np.nan_to_num(-y*log(a)-(1-y)*log(1-a)))\n",
    "    nabla_b = [zeros(size(b)) for b in net.biases]\n",
    "    nabla_w = [zeros(size(w)) for w in net.weights]\n",
    "    for (x, y) in mini_batch\n",
    "        delta_nabla_b, delta_nabla_w = backprop(net,x, y)\n",
    "        nabla_b = [nb+dnb for (nb, dnb) in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw+dnw for (nw, dnw) in zip(nabla_w, delta_nabla_w)]\n",
    "    end\n",
    "    net.weights = [w-(eta/size(mini_batch,1))*nw for (w, nw) in zip(net.weights, nabla_w)]\n",
    "    net.biases = [b-(eta/size(mini_batch,1))*nb for (b, nb) in zip(net.biases, nabla_b)]\n",
    "    \n",
    "    return net.weights,net.biases\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backprop(net,x, y)\n",
    "    nabla_b = [zeros(size(b)) for b in net.biases]\n",
    "    nabla_w = [zeros(size(w)) for w in net.weights]\n",
    "    # feedforward\n",
    "    activation = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    for (b, w) in zip(net.biases, net.weights)\n",
    "        #z = dot(w, activation)+b\n",
    "        z = (w*activation)+b\n",
    "        push!(zs,z)\n",
    "        activation = sigmoid.(z)\n",
    "        push!(activations,activation)\n",
    "    end\n",
    "    # backward pass\n",
    "    delta1 = Array(cost_derivative(net,activations[end], y))\n",
    "    delta=[]\n",
    "    for i in delta1\n",
    "        push!(delta,i)\n",
    "    end\n",
    "    nabla_b[end] = delta\n",
    "    nabla_w[end] = delta*transpose(activations[end-1])\n",
    "    # Note that the variable l in the loop below is used a little\n",
    "    # differently to the notation in Chapter 2 of the book.  Here,\n",
    "    # l = 1 means the last layer of neurons, l = 2 is the\n",
    "    # second-last layer, and so on.  It's a renumbering of the\n",
    "    # scheme in the book, used here to take advantage of the fact\n",
    "    # that Python can use negative indices in lists.\n",
    "    for l=net.numOfLayers-1:-1:2\n",
    "        z = zs[l-1]\n",
    "        #println(size(net.weights[l]), \" \", size(delta),\" \", size(sp))\n",
    "        delta = (transpose(net.weights[l])*delta) \n",
    "        nabla_b[l-1] = delta\n",
    "        nabla_w[l-1] = (delta * transpose(activations[l-1]))\n",
    "    end\n",
    "    return nabla_b, nabla_w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross_entropy (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cross_entropy(a,y)\n",
    "     return (-y*log(a)-(1-y)*log(1-a))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evaluate(net,test_data)\n",
    "    test_results = [(findmax(feedforward(net,x))[2], findmax(y)[2]) for (x, y) in test_data]\n",
    "    #println(test_results)\n",
    "    return  sum((x == y) for (x, y) in test_results) / size(test_data)[1] \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cost_derivative (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cost_derivative(net,output_activations, y)\n",
    "    return output_activations-y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectorized_result (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function vectorized_result(j)\n",
    "    e = zeros(10, 1)\n",
    "    e[j+1] = 1.0\n",
    "    return e\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Iterators.Zip2{Array{Any,1},Array{Array{Float64,2},1}}(Any[[0.0, 0.0, 73.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 194.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 249.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 208.0, 0.0], [0.0, 9.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  41.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 0.0], [0.0, 92.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  132.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 67.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  213.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  254.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  253.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 254.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 228.0, 0.0, 0.0, 0.0, 254.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 121.0, 54.0, 0.0, 0.0, 0.0, 0.0, 0.0]  …  [0.0, 0.0, 0.0, 0.0, 0.0, 229.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 25.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 229.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 80.0, 20.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 253.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 107.0, 0.0, 0.0], [0.0, 253.0, 40.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 191.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 254.0, 0.0, 0.0], [0.0, 253.0, 0.0, 0.0, 0.0, 225.0, 0.0, 0.0, 0.0, 9.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 47.0, 54.0, 0.0, 0.0], [0.0, 161.0, 0.0, 0.0, 0.0, 255.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.0, 0.0, 0.0]], Array{Float64,2}[[0.0; 0.0; … ; 0.0; 0.0], [0.0; 0.0; … ; 0.0; 0.0], [0.0; 1.0; … ; 0.0; 0.0], [1.0; 0.0; … ; 0.0; 0.0], [0.0; 0.0; … ; 0.0; 0.0], [0.0; 1.0; … ; 0.0; 0.0], [0.0; 0.0; … ; 0.0; 0.0], [0.0; 0.0; … ; 0.0; 1.0], [0.0; 0.0; … ; 0.0; 0.0], [0.0; 0.0; … ; 0.0; 1.0]  …  [0.0; 0.0; … ; 0.0; 0.0], [0.0; 0.0; … ; 1.0; 0.0], [0.0; 0.0; … ; 0.0; 1.0], [1.0; 0.0; … ; 0.0; 0.0], [0.0; 1.0; … ; 0.0; 0.0], [0.0; 0.0; … ; 0.0; 0.0], [0.0; 0.0; … ; 0.0; 0.0], [0.0; 0.0; … ; 0.0; 0.0], [0.0; 0.0; … ; 0.0; 0.0], [0.0; 0.0; … ; 0.0; 0.0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_inp=reshape(trainX,(60000,784))\n",
    "training_inputs=[]\n",
    "for i=1:size(training_inp)[1]\n",
    "    push!(training_inputs,training_inp[i,:])\n",
    "end\n",
    "training_outputs = [vectorized_result(y) for y in trainY]\n",
    "training_data = zip(training_inputs, training_outputs)\n",
    "\n",
    "test_inp=reshape(testX,(10000,784))\n",
    "test_inputs=[]\n",
    "for i=1:size(test_inp)[1]\n",
    "    push!(test_inputs,test_inp[i,:])\n",
    "end\n",
    "test_outputs = [vectorized_result(y) for y in testY]\n",
    "test_data = zip(test_inputs, test_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 0.098  epoch 2: 0.098  epoch 3: 0.098  epoch 4: 0.098  "
     ]
    }
   ],
   "source": [
    "SGD(net,training_data, 100 , 30 , 0.5, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Note I already tried running 30 epochs and network was stuck at the same accuracy but the results were not saved when I closed the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
